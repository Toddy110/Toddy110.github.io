{"meta":{"title":"Modesty.","subtitle":"Stay hungry, stay foolish","description":"","author":"ToddyN","url":"https://toddy110.github.io","root":"/"},"pages":[{"title":"404","date":"2025-09-19T06:04:56.000Z","updated":"2025-09-19T06:05:57.189Z","comments":true,"path":"404/index.html","permalink":"https://toddy110.github.io/404/index.html","excerpt":"","text":""},{"title":"contact","date":"2025-09-19T06:47:51.000Z","updated":"2025-09-19T06:59:00.984Z","comments":true,"path":"contact/index.html","permalink":"https://toddy110.github.io/contact/index.html","excerpt":"","text":"欢迎来到我的留言板！ 在这里，您可以： 💬 留下您的想法和建议 🤝 申请友情链接 📧 与我交流技术问题 💡 分享有趣的想法 请在下方评论区留言，我会尽快回复！"},{"title":"About","date":"2025-10-10T09:30:55.703Z","updated":"2025-10-10T09:30:55.703Z","comments":false,"path":"about/index.html","permalink":"https://toddy110.github.io/about/index.html","excerpt":"","text":"这里是我的个人简介页。 名称：ToddyN 职业：Student 格言：Stay hungry, stay foolish 如果你看到这里，说明 About 页面已经配置成功啦。"},{"title":"categories","date":"2025-09-08T01:00:42.000Z","updated":"2025-09-08T01:01:33.000Z","comments":true,"path":"categories/index.html","permalink":"https://toddy110.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2025-09-08T01:05:16.000Z","updated":"2025-09-08T01:06:40.000Z","comments":true,"path":"tags/index.html","permalink":"https://toddy110.github.io/tags/index.html","excerpt":"","text":""},{"title":"friends","date":"2025-10-10T04:00:00.000Z","updated":"2025-10-10T08:53:28.659Z","comments":true,"path":"friends/index.html","permalink":"https://toddy110.github.io/friends/index.html","excerpt":"","text":"欢迎来到友情链接页，这里展示一些我推荐的站点"}],"posts":[{"title":"数据结构-绪论","slug":"数据结构-绪论","date":"2025-10-11T08:48:12.000Z","updated":"2025-10-12T07:58:03.067Z","comments":true,"path":"2025/10/11/数据结构-绪论/","permalink":"https://toddy110.github.io/2025/10/11/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%BB%AA%E8%AE%BA/","excerpt":"1. 基本概念和术语数据 (Data)定义 | 数据 (Data): 数据是信息的载体, 是描述客观事物属性的数, 字符及所有能输入到计算机中并被计算机程序识别和处理的符号的集合. 对于计算机来说, 就是二进制的 0 和 1 数值型数据: 整数, 定点数, 浮点数非数值型数据: 文字数据","text":"1. 基本概念和术语数据 (Data)定义 | 数据 (Data): 数据是信息的载体, 是描述客观事物属性的数, 字符及所有能输入到计算机中并被计算机程序识别和处理的符号的集合. 对于计算机来说, 就是二进制的 0 和 1 数值型数据: 整数, 定点数, 浮点数非数值型数据: 文字数据 数据元素 (Data Element)定义 | 数据元素 (Data Element): 是数据的基本单位, 通常作为一个整体进行考虑和处理 (最小单位是 bit). 数据项 (Data Item)定义 | 数据项 (Data Item): 一个数据元素可由若干数据项组成, 数据项是构成数据元素的不可分割的最小单位. 我们要根据实际的业务需求来确定什么是数据元素, 什么是数据项 数据元素又称为元素, 结点, 记录 (record) 数据结构 (Data Structure)结构: 各个元素之间的关系 (relationships among elements) 数据结构 (Data Structure): 形式定义: 某一数据对象的所有数据成员之间的关系. 记为:Data_Structure={D,S} \\text{Data\\_Structure} = \\{ D, S \\} Data_Structure={D,S} 其中, D 是某一数据对象, S 是该对象中所有数据成员之间的关系的有限集合. 例: 复数的数据结构定义 (complex number) 数据对象 (Data Object)数据对象 (Data Object): 具有相同性质的数据元素的集合, 是数据的一个子集 整数数据对象 N={0,±1,±2,… }N = \\{0,\\pm1, \\pm2, \\dots\\}N={0,±1,±2,…} 字符数据对象 C={′A′,′B′,′C′,⋯ ,′F′}C = \\{'A','B','C',\\cdots, 'F'\\}C={′A′,′B′,′C′,⋯,′F′} 俱乐部会员表数据对象 (club member table) 数据处理 (Data Processing)数据处理 (Data Processing): 将数据通过人力或机器, 将收集到的数据加以系统的处理, 归纳出有价值的信息 排序, 归并, 编辑, 计算, 查找, 查询, 分类, 变换等 数据结构 (Data Structure) 的三要素 (Three Elements)(1) 逻辑结构 (Logical Structure) — 数据结构之间的逻辑关系是什么 从逻辑关系上描述数据, 与数据的存储无关; 从具体问题抽象出来的数据模型 (data model) 与数据元素本身的形式, 内容无关 数据的逻辑结构分类 线性结构: 线性表, 栈 (stack), 队列 (queue), 数组 (array), 串 (string) 非线性结构: 树 (tree), 图 (graph 或 network), 广义表, 多维数组 四个基本结构:集合 (Set): 各个元素同属一个集合, 别无其他关系 线性结构 (Linear): 数据元素之间是一对一的关系. 除了第一个元素, 所有元素都有唯一前驱; 除了最后一个元素, 所有元素都有唯一后继 树形结构 (Tree): 数据元素之间是一对多的关系 网状结构 (Graph): 数据元素之间是多对多的关系 (2) 物理结构 (Physical Structure) — 如何用计算机表示数据元素的逻辑关系顺序存储 (Sequential Storage): 把逻辑上相邻的元素存储在物理位置上也相邻的存储单元中, 元素之间的关系由存储单元的邻接关系来体现 链式存储 (Linked Storage): 逻辑上相邻的元素在物理位置上可以不相邻, 借助指示元素存储地址的指针来表示元素之间的逻辑关系 索引存储 (Indexed Storage): 在存储元素信息的同时, 还建立附加的索引表. 索引表中的每项称为索引项, 索引项的一般形式是 (关键字, 地址) 散列存储 (Hash Storage): 根据元素的关键字直接计算出该元素的存储地址, 又称哈希 (Hash) 存储 若采用顺序存储, 则各个数据元素在物理上必须是连续的; 若采用非顺序存储, 则各个元素在物理上可以是离散的 数据的存储结构会影响存储空间分配的方便程度 数据的存储结构会影响对数据运算的速度 存储方式 逻辑相邻是否物理相邻 额外信息 优点 缺点 顺序存储 (Sequential) 是 无 随机访问快 插入/删除慢 链式存储 (Linked) 否 指针/引用 插入/删除快 额外指针开销 索引存储 (Indexed) 否 索引表 查找加速 维护索引成本 散列存储 (Hash) 否 哈希函数 访问接近 O(1) 冲突与退化 (3) 数据的运算 (Operations) — 施加在数据上的运算包括运算的定义和实现运算的定义是针对逻辑结构的, 运算的实现是针对存储结构的, 指出运算的具体操作步骤 抽象数据类型 (Abstract Data Type, ADT)数据类型 (Data Type): 一个值的集合和定义在这个值集上的一组操作的总称 C 语言中的基本数据类型: int (整型), char (字符型), float (浮点型), double (双精度型), void (无值) 抽象数据类型 (Abstract Data Type, ADT): 是指一个数学模型以及定义在此数学模型上的一组操作 逻辑特性, 与在计算机内的表示与实现无关抽象数据类型 === 数据结构 +++ 定义在此数据结构上的一组操作 矩阵的抽象数据类型: 矩阵 + (求转置, 加, 乘, 求逆, 求特征值) //complex.h //隐藏数据表示，不提供具体的说明 typedef struct complex *Complex; Complex COMPLEXinit(float , float); float Re(Complex); float Im(Complex); Complex COMPLEXmult(Complex, Complex); Complex COMPLEXadd(Complex, Complex); void showComplex(Complex ); 抽象数据类型的描述: 抽象数据类型可用(D,P,S)三元组表示 D是数据对象 S是D上的关系集 P是对D的基本操作集 ADT 抽象数据类型名 { 数据对象：〈数据对象的定义〉 数据关系：〈数据关系的定义〉 基本操作：〈基本操作的定义〉 } ADT 抽象数据类型名 数据对象,数据关系用伪码描述;基本操作定义格式为: 基本操作名（参数表） 初始条件：〈初始条件描述〉 操作结果：〈操作结果描述〉 基本操作有两种参数:赋值参数只为操作提供输入值;引用参数以&amp;打头,除可提供输入值外,还将返回操作结果 “初始条件”描述了操作执行之前数据结构和参数应满足的条件,若不满足,则操作失败,并返回相应出错信息 “操作结果”说明了操作正常完成之后,数据结构的变化状况和应返回的结果. 抽象数据类型的表示和实现:抽象数据类型可以通过固有数据类型（高级编程语言中已实现的数据类型）来实现： 抽象数据类型 数据对象 基本操作 类 class 数据成员 成员函数（方法） 在C++中,类的成分(数据成员和成员函数)可以有三种访问级别: private 私有成分(只允许类的成员函数进行访问) protected 保护成分(只允许类的成员函数及其子孙类进行访问) public 公有成分(允许类的成员函数,类的实例及其子孙类,子孙类的实例进行访问) 2. 程序的产生五个阶段: 需求(输入,输出) 设计(编写算法) 分析(选择最佳算法) 细化与编码(编写程序) 验证(程序验证、测试、调试) 算法分析算法定义为了解决某类问题而规定的一个有限长的操作序列 特性 有穷性:算法在执行有穷步后能结束 确定性:每步定义都是确切,无歧义 可行性:每一条运算应足够基本 输入:有0个或多个输入 输出:有一个或多个输出 算法设计例子:选择排序 (Selection Sort)问题:递增排序解决方案:逐个选择最小数据代码如下: void SelectSort(int a[], int n) { for (int i = 0; i &lt; n - 1; i++) { int k = i; // 从a[i]查到a[n-1], 找最小整数, 在a[k] for (int j = i + 1; j &lt; n; j++) { if (a[i] &lt; a[j]) { k = j; a[i] ^= a[j]; a[j] ^= a[i]; a[i] ^= a[j];// 邪修,用来交换数值 } } } } 性能分析与度量评价标准算法的评价标准: 正确性:包括不含语法错误,对几组数据运行正确,对典型,苛刻的数据运行正确,对所有数据运行正确 可读性: 效率:高效,低存储需要.(算法执行时间短,同时所占用的存储空间小) 健壮性:当输入非法数据时,算法也能作出适当反应,而不会出现莫名其妙的输出结果 时间复杂度 (Time Complexity)后期测试(实测)算法的后期测试:在算法中的某些部位插装时间函数time(),测定算法完成某一功能所花费时间 double start, stop; time (&amp;start); int k = seqsearch (a, n, x); time (&amp;stop); double runTime = stop - start; printf (\" %d%d\\n \" , n, runTime); 事前估计(分析)算法的事前估计: 运行时间 = 算法中每条语句执行时间之和 每条语句执行时间 = 该语句的执行次数(频度)* 语句执行一次所需时间 语句执行一次所需时间取决于机器的指令性能和速度和编译所产生的代码质量,很难确定 设每条语句执行一次所需时间为单位时间,则一个算法的运行时间就是该算法中所有语句的频度之和 定义与表示法时间复杂度:算法中语句重复执行次数的数量级是时间复杂度.表示方法: T(n)=O(f(n)) T(n) = O(f(n)) T(n)=O(f(n)) T(n)T(n)T(n)称做渐进时间复杂度,简称时间复杂度 f(n)f(n)f(n)表示基本操作重复执行的次数,是nnn的某个函数,随问题规模nnn的增大,算法执行时间的增长率和f(n)f(n)f(n)的增长率属于同一数量级 OOO表示f(n)f(n)f(n)和T(n)T(n)T(n)只相差一个常数倍 示例 1:常数时间打印(O(1))下面看两个代码.代码一: void print(int a[]) { int i = 0; cout &lt;&lt; a[i]; i++; cout &lt;&lt; a[i]; } 结论:时间复杂度为O(1)O(1)O(1) 示例 2:线性时间打印(O(n))代码二: void print (int a[], int n) { int i = 0; for(i = 0;i &lt; n; i++) cout &lt;&lt; a[i]; } 结论:时间复杂度为O(n)O(n)O(n) 示例 3:顺序查找 (Sequential Search, 平均复杂度 O(n))再来看一个代码: int seqsearch(int a[], int n, int x) { // 在a[0],...,a[n - 1]中搜索x int i = 0; while(i &lt; n &amp;&amp; a[i] != x) i++; if(i == n) return -1; return i; } 平均操作次数为(1+2+3+⋯+n+n)∗1n+1=n(n+3)2(n+1)(1 + 2 + 3 + \\cdots + n + n) * \\frac{1}{n+1} = \\frac{n(n+3)}{2(n+1)}(1+2+3+⋯+n+n)∗n+11​=2(n+1)n(n+3)​, 时间复杂度为O(n)O(n)O(n) 空间复杂度 (Space Complexity) 存储空间的固定部分:程序指令代码的空间,常数,简单变量,定长成分(如数组元素,结构成分,对象的数据成员等)变量所占空间 可变部分:尺寸与实例特性有关的成分变量所占空间,引用变量所占空间,递归栈所用空间,通过new和delete命令动态使用空间 几种时间复杂度 O(1)O(1)O(1):常数时间 O(log2n)O(log_{2}{n})O(log2​n):对数时间 O(n)O(n)O(n):线性时间 O(nlog2n)O(nlog_{2}{n})O(nlog2​n):线性对数时间 O(n2)O(n^2)O(n2):平方时间 O(n3)O(n^3)O(n3):立方时间 O(2n)O(2^n)O(2n):指数时间 上述的时间复杂度的优劣次序如下(n≥16)(n \\geq 16)(n≥16): O(1)&lt;O(log2n)&lt;O(n)&lt;O(nlog2n)&lt;O(n2)&lt;O(n3)&lt;O(2n) O(1)&lt;O(log_{2}{n})&lt;O(n)&lt;O(nlog_{2}{n})&lt;O(n^2)&lt;O(n^3)&lt;O(2^n) O(1)&lt;O(log2​n)&lt;O(n)&lt;O(nlog2​n)&lt;O(n2)&lt;O(n3)&lt;O(2n) 重要知识点: 算法的计算量的大小称为计算的复杂性 链接存储表示中数据元素之间的逻辑关系是由指针表示的 算法的时间复杂度取决于问题的规模和待处理数据的状态 在数据结构中，与所使用的计算机无关的是数据的逻辑结构 数据元素是数据的基本单位，而不是最小单位 ;最小单位是bit 数据的不可分割的最小单位是数据项","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://toddy110.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://toddy110.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"CNN基础","slug":"CNN基础","date":"2025-10-01T04:50:12.000Z","updated":"2025-10-12T05:40:14.001Z","comments":true,"path":"2025/10/01/CNN基础/","permalink":"https://toddy110.github.io/2025/10/01/CNN%E5%9F%BA%E7%A1%80/","excerpt":"","text":"CNN (Convolutional Neural Network, 卷积神经网络) (基础) 1. 图像基础与本质1.1 图像的输入结构以MNIST数据集为例, 灰度图像的输入尺寸为 1×28×281 \\times 28 \\times 281×28×28, 即: 通道数 (Channel): 1, 表示灰度图像只有一个颜色通道; 宽度 (Width): 28, 图像的宽度为28像素; 高度 (Height): 28, 图像的高度为28像素. 在实际应用中, 彩色图像通常包含红 (R), 绿 (G), 蓝 (B) 三个通道 (channel). 1.2 数字图像的本质数字图像 (digital image) 本质上是由像素 (Pixel) 组成的二维矩阵, 每个像素记录了对应位置的光强或颜色信息. 在数字成像系统 (digital imaging system) 中, 常用光敏电阻 (Photoresistor) 或光电二极管 (Photodiode) 作为感光元件. 光敏电阻的电阻值会随着光照强度的变化而变化. 通过在电路中测量光敏电阻两端的电压和电流, 可以计算出其电阻值, 进而推算出该位置的光强. 成像时, 通常会使用透镜系统将外部光线聚焦到感光元件阵列上. 每个光敏电阻对应一个光锥, 接收器尺寸越小, 光锥角度越小, 能够采集的空间分辨率越高. 多个光敏电阻按照规则排布, 形成感光阵列, 每个光敏电阻即为一个像素. 整个阵列即可采集一幅完整的图像. 彩色图像通常在每个像素位置上设置多个不同颜色滤光片 (如红, 绿, 蓝), 分别采集不同波段的光强, 实现彩色成像. 在实际的彩色数字成像设备中, 最常见的滤色片排列方式是拜耳阵列 (Bayer Pattern), 其中最常见的是RGGB排列. RGGB表示每2×2的像素块中有2个绿色, 1个红色和1个蓝色滤光片, 排列如下: 红 (R) 绿 (G) 绿 (G) 蓝 (B) 这种设计是因为人眼对绿色更敏感, 因此绿色像素数量更多. 通过拜耳阵列采集到的原始数据, 经过插值算法 (去马赛克, Demosaicing) 后, 可以还原出全分辨率的彩色图像. 因此, 数字图像的本质是对空间中光强分布的离散采样和数字化表达. 分辨率越高, 能够还原的细节越丰富. 1.3 栅格图像与矢量图像 栅格图像 (Raster Image): 由像素 (Pixel) 组成的二维矩阵, 每个像素有固定的颜色或灰度值. 常见格式有JPEG, PNG, BMP等. 优点是能够表现丰富的细节和复杂的色彩变化, 适合照片, 扫描图像等. 缺点是放大后会出现锯齿 (失真), 文件体积较大. 矢量图像 (Vector Image): 由点, 线, 曲线和多边形等几何图形通过数学公式描述. 常见格式有SVG, EPS, PDF等. 优点是无论放大多少倍都不会失真, 文件体积通常较小, 适合图标, 标志, 插画等. 缺点是不适合表现复杂的色彩渐变和细节丰富的照片. 应用场景: 栅格图像常用于数码摄影, 医学影像, 遥感等需要表现真实世界细节的场合. 矢量图像常用于平面设计, 排版, CAD制图等需要高精度缩放和清晰边界的场合. 2. 卷积操作与特征提取2.1 卷积操作的基本原理在进行卷积操作时, 通常会在图像上选取一个小块 (patch), 其形状为 3×H′×W′3 \\times H' \\times W'3×H′×W′, 其中3表示输入的通道数 (如RGB三通道), H′H'H′ 和 W′W'W′ 分别为该块的高度和宽度. 这个小块会作为卷积核的感受野, 在整张图像上以滑动窗口的方式移动, 遍历所有位置. 每次滑动时, 对当前区域进行卷积计算, 得到一个输出值. 所有位置的输出值拼接在一起, 形成新的特征图 (feature map). 卷积操作完成后, 输出的通道数, 宽度和高度通常会发生变化. 每个输出通道 (output channel) 都综合了输入patch中的全部信息, 代表了不同的特征响应. 2.2 卷积层的结构与流程 图像首先经过卷积层 (convolutional layer), 如 5×55 \\times 55×5 卷积核, 可以得到4个通道, 每个通道 24×2424 \\times 2424×24 的特征图 (feature map, 记作 C1C_1C1​). 卷积层的作用是提取图像的空间特征. 卷积操作后, 输出张量依然保持三维结构 (通道数 × 宽度 × 高度), 但通道数, 宽度和高度可能发生变化. 与全连接层 (fully connected layer) 不同, 全连接层会将图像展平成一维向量, 丧失原有的空间结构信息. 卷积层输出的特征图通常会经过下采样 (subsampling/pooling), 如 2×22 \\times 22×2 池化, 得到新的特征图 (S1S_1S1​), 例如4个通道, 每个通道 12×1212 \\times 1212×12. 下采样操作会减小宽度和高度, 通道数保持不变, 主要目的是减少数据量, 降低计算复杂度. 可以继续堆叠卷积层和下采样层. 例如, 再经过一个 5×55 \\times 55×5 卷积, 得到 C2C_2C2​ (8个通道, 8×88 \\times 88×8), 再下采样一次, 得到 S2S_2S2​ (8个通道, 4×44 \\times 44×4). 最后,将三阶张量(如 8×4×48 \\times 4 \\times 48×4×4)展平成一维向量(通过 view 操作),再通过全连接层映射到十维输出,完成分类任务.常用交叉熵损失(cross-entropy loss)和 softmax 进行概率分布计算. 总结:构建神经网络时,需要明确输入和输出张量的维度,并通过不同的层结构将其映射到目标空间,实现特征提取与分类(Feature Extraction + Classification). 3. 卷积操作的数学与代码示例3.1 单通道卷积计算过程以 5×55 \\times 55×5 的输入矩阵和 3×33 \\times 33×3 的卷积核为例,演示卷积操作的计算过程: 输入(Input): 3 4 6 5 7 2 4 6 8 2 1 6 7 8 4 9 7 4 6 2 3 7 5 4 1 卷积核(Kernel): 1 2 3 4 5 6 7 8 9 输出(Output): 211 295 262 259 282 214 251 253 169 详细计算过程: 左上角输出(第1行第1列): 3×1+4×2+6×3+2×4+4×5+6×6+1×7+6×8+7×9=2113\\times1 + 4\\times2 + 6\\times3 + 2\\times4 + 4\\times5 + 6\\times6 + 1\\times7 + 6\\times8 + 7\\times9 = 2113×1+4×2+6×3+2×4+4×5+6×6+1×7+6×8+7×9=211 第1行第2列: 4×1+6×2+5×3+4×4+6×5+8×6+6×7+7×8+8×9=2954\\times1 + 6\\times2 + 5\\times3 + 4\\times4 + 6\\times5 + 8\\times6 + 6\\times7 + 7\\times8 + 8\\times9 = 2954×1+6×2+5×3+4×4+6×5+8×6+6×7+7×8+8×9=295 第1行第3列: 6×1+5×2+7×3+6×4+8×5+2×6+7×7+8×8+4×9=2626\\times1 + 5\\times2 + 7\\times3 + 6\\times4 + 8\\times5 + 2\\times6 + 7\\times7 + 8\\times8 + 4\\times9 = 2626×1+5×2+7×3+6×4+8×5+2×6+7×7+8×8+4×9=262 第2行第1列: 2×1+4×2+6×3+1×4+6×5+7×6+9×7+7×8+4×9=2592\\times1 + 4\\times2 + 6\\times3 + 1\\times4 + 6\\times5 + 7\\times6 + 9\\times7 + 7\\times8 + 4\\times9 = 2592×1+4×2+6×3+1×4+6×5+7×6+9×7+7×8+4×9=259 第2行第2列: 4×1+6×2+8×3+6×4+7×5+8×6+7×7+4×8+6×9=2824\\times1 + 6\\times2 + 8\\times3 + 6\\times4 + 7\\times5 + 8\\times6 + 7\\times7 + 4\\times8 + 6\\times9 = 2824×1+6×2+8×3+6×4+7×5+8×6+7×7+4×8+6×9=282 第2行第3列: 6×1+8×2+2×3+7×4+8×5+4×6+4×7+6×8+2×9=2146\\times1 + 8\\times2 + 2\\times3 + 7\\times4 + 8\\times5 + 4\\times6 + 4\\times7 + 6\\times8 + 2\\times9 = 2146×1+8×2+2×3+7×4+8×5+4×6+4×7+6×8+2×9=214 第3行第1列: 1×1+6×2+7×3+9×4+7×5+4×6+3×7+7×8+5×9=2511\\times1 + 6\\times2 + 7\\times3 + 9\\times4 + 7\\times5 + 4\\times6 + 3\\times7 + 7\\times8 + 5\\times9 = 2511×1+6×2+7×3+9×4+7×5+4×6+3×7+7×8+5×9=251 第3行第2列: 6×1+7×2+8×3+7×4+4×5+6×6+7×7+5×8+4×9=2536\\times1 + 7\\times2 + 8\\times3 + 7\\times4 + 4\\times5 + 6\\times6 + 7\\times7 + 5\\times8 + 4\\times9 = 2536×1+7×2+8×3+7×4+4×5+6×6+7×7+5×8+4×9=253 第3行第3列: 7×1+8×2+4×3+4×4+6×5+2×6+5×7+4×8+1×9=1697\\times1 + 8\\times2 + 4\\times3 + 4\\times4 + 6\\times5 + 2\\times6 + 5\\times7 + 4\\times8 + 1\\times9 = 1697×1+8×2+4×3+4×4+6×5+2×6+5×7+4×8+1×9=169 注:每个输出值对应卷积核在输入矩阵上滑动到不同位置时,patch与kernel对应元素相乘后求和的结果. 3.2 多通道卷积的结构与参数在卷积神经网络中,多通道卷积的本质是:每个输出通道都通过对所有输入通道分别进行卷积,再将结果按元素相加得到. 更为严谨地说,假设输入特征图的通道数为 CinC_{in}Cin​,输出特征图的通道数为 CoutC_{out}Cout​,卷积核的空间尺寸为 Kh×KwK_h \\times K_wKh​×Kw​.那么,卷积核的权重张量形状为: (out_channels, in_channels, kernel_size_h, kernel_size_w) 其中,kernel_size_h 和 kernel_size_w 分别表示卷积核的高度和宽度. 具体计算流程如下: 每个输出通道的生成 对于每一个输出通道 o (o=1,2,…,C_out),都对应有 C_in 个二维卷积核(每个输入通道一个). 对于每个输入通道 i (i=1,2,…,C_in),用该通道的输入特征图与对应的卷积核进行二维卷积,得到一个中间特征图. 将所有 C_in 个中间特征图按元素相加(逐元素求和),得到该输出通道的最终特征图. 卷积核权重的组织 整个卷积层的权重是一个四维张量,形状为 (out_channels, in_channels, kernel_size_h, kernel_size_w). 其中,第 o 个输出通道的卷积核权重为 W[o, :, :, :],包含了对所有输入通道的卷积核. 输出张量的形状 假设输入张量形状为 (N, C_in, H_in, W_in),其中 N 是批量大小. 输出张量的形状为 (N, C_out, H_out, W_out),其中 H_out 和 W_out 由输入尺寸,卷积核尺寸,步幅,填充等参数共同决定. 举例说明 以常见的 RGB 图像为例,C_in=3.若设置 C_out=16,则该卷积层共有 16×3=48 个二维卷积核,每个输出通道都融合了所有输入通道的信息. 总结: 每个输出通道都聚合了所有输入通道的卷积结果. 卷积核的四维结构确保了输入通道和输出通道之间的全连接. 这种设计使得网络能够学习到跨通道的复杂特征组合. 因此,PyTorch 等深度学习框架中,Conv2d 层的权重参数形状为 (out_channels, in_channels, kernel_size_h, kernel_size_w),严格对应上述数学描述. 3.3 代码示例:多通道卷积由此,我们可以写出卷积操作的示例代码: import torch in_channels, out_channels = 5, 10 width, height = 100, 100 kernel_size = 3 batch_size = 1 input = torch.randn(batch_size, in_channels, width, height) conv_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size = kernel_size) output = conv_layer(input) print(input.shape) print(output.shape) print(conv_layer.weight.shape) 下面对上述代码中的各个参数和操作进行有条理的说明: 输入输出通道数 in_channels:表示输入张量的通道数.例如,对于RGB彩色图像,in_channels=3;对于灰度图像,in_channels=1.本例中设为5,表示输入有5个通道. out_channels:表示卷积操作后输出张量的通道数.这个值由我们自行设定,通常越大,网络能够学习到的特征越丰富.本例中设为10. 图像尺寸 width 和 height:分别表示输入图像的宽度和高度.本例中均为100. 卷积核大小 kernel_size:指定卷积核的空间尺寸.可以是单个整数(表示正方形卷积核),也可以是二元组(如(3, 5),表示非正方形卷积核).本例中为3,表示 3×33 \\times 33×3 的卷积核. 输入张量的生成 input = torch.randn(batch_size, in_channels, width, height):生成一个形状为 (batch_size, in_channels, width, height) 的四维张量,元素服从标准正态分布.这里 batch_size=1,表示一次只输入一张多通道图像. 卷积层的定义 conv_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size):创建一个二维卷积层.需要指定输入通道数,输出通道数和卷积核大小这三个核心参数. 注意:卷积核的形状不一定要是正方形(即width=height),但实际应用中常用正方形卷积核(如 3×33 \\times 33×3,5×55 \\times 55×5 等),因为其在空间上具有对称性,便于特征提取. 通过上述参数的设定,可以灵活地构建适用于不同输入数据和任务需求的卷积层结构. 输出结果如下: torch.Size([1, 5, 100, 100]) torch.Size([1, 10, 98, 98]) torch.Size([10, 5, 3, 3]) 4. 卷积操作的参数详解4.1 Padding(填充)Padding(填充)用于在输入特征图的边缘补零,以控制输出特征图的空间尺寸. 公式: padding = (kernel_size - 1) / 2 其中 kernel_size 是卷积核的尺寸(如 3,5,7 等).如果卷积核尺寸为奇数,这个公式可以保证输出尺寸与输入尺寸一致. 代码示例:Paddingimport torch input = [3, 4, 6, 5, 7, 2, 4, 6, 8, 2, 1, 6, 7, 8, 4, 9, 7, 4, 6, 2, 3, 7, 5, 4, 1] input = torch.Tensor(input).view(1, 1, 5, 5) conv_layer = torch.nn.Conv2d(1, 1, kernel_size = 3, padding = 1) kernel = torch.Tensor([1, 2, 3, 4, 5, 6, 7, 8, 9]).view(1, 1, 3, 3) conv_layer.weight.data = kernel.data output = conv_layer(input) print(output) 上述代码演示了如何在 PyTorch 中手动设置输入,卷积核,并使用 padding 进行卷积操作. 首先,定义了一个 5×55 \\times 55×5 的输入矩阵,并用 torch.Tensor(...).view(1, 1, 5, 5) 变成四维张量,符合卷积层的输入格式(batch size, channel, height, width). 创建了一个 3×33 \\times 33×3 的卷积核,并手动赋值给卷积层的权重. padding=1 表示在输入的每一边都补上一圈0,使得输出的空间尺寸与输入一致. 最后,将输入送入卷积层,输出结果的空间尺寸仍为 5×55 \\times 55×5,验证了 padding 的作用. 通过这个例子可以直观理解 padding 如何影响卷积输出的尺寸. 输出结果如下: tensor([[[[ 90.9643, 167.9643, 223.9643, 214.9643, 126.9643], [113.9643, 210.9643, 294.9643, 261.9643, 148.9643], [191.9643, 258.9643, 281.9643, 213.9643, 121.9643], [193.9643, 250.9643, 252.9643, 168.9643, 85.9643], [ 95.9643, 111.9643, 109.9643, 67.9643, 30.9643]]]], grad_fn=&lt;ConvolutionBackward0&gt;) 4.2 Stride(步幅)Stride(步幅)用于控制卷积核在输入特征图上每次移动的距离. stride=2 表示卷积核每次在输入特征图上移动2个像素(而不是默认的1个像素). 这样会导致输出特征图的宽度和高度都变小,相当于对输入做了下采样. 通过设置不同的stride,可以灵活控制输出特征图的空间尺寸. 本例中,输入为 5×55 \\times 55×5,卷积核为 3×33 \\times 33×3,stride=2,输出的空间尺寸会比stride=1时更小. 代码示例:Strideimport torch input = [3, 4, 6, 5, 7, 2, 4, 6, 8, 2, 1, 6, 7, 8, 4, 9, 7, 4, 6, 2, 3, 7, 5, 4, 1] input = torch.Tensor(input).view(1, 1, 5, 5) conv_layer = torch.nn.Conv2d(1, 1, kernel_size = 3, stride = 2, bias = False) kernel = torch.Tensor([1, 2, 3, 4, 5, 6, 7, 8, 9]).view(1, 1, 3, 3) conv_layer.weight.data = kernel.data output = conv_layer(input) print(output) 上述代码演示了stride(步幅)参数的作用: stride=2 表示卷积核每次在输入特征图上移动2个像素. 这样会导致输出特征图的宽度和高度都变小,相当于对输入做了下采样. 通过设置不同的stride,可以灵活控制输出特征图的空间尺寸. 本例中,输入为 5×55 \\times 55×5,卷积核为 3×33 \\times 33×3,stride=2,输出的空间尺寸会比stride=1时更小. 通过这个例子可以直观理解stride对卷积输出尺寸的影响. 输出结果如下: tensor([[[[211., 262.], [251., 169.]]]], grad_fn=&lt;ConvolutionBackward0&gt;) 5. 下采样(Pooling)操作5.1 MaxPooling(最大池化)MaxPooling(最大池化)是一种常用的下采样方法.对于 2×22 \\times 2 2×2 的MaxPooling,默认stride = 2. 其原理是在每个 2×22 \\times 22×2 区域内取最大值,减小特征图尺寸,通道数保持不变. Maxpooling只能在一个通道内做,通道之间是无法做Maxpooling的(通道数量不变,图像大小变化). 代码示例:MaxPoolingimport torch input = [3, 4, 6, 5, 2, 4, 6, 8, 1, 6, 7, 8, 9, 7, 4, 6] input = torch.Tensor(input).view(1, 1, 4, 4) maxpooling_layer = torch.nn.MaxPool2d(kernel_size = 2) output = maxpooling_layer(input) print(output) 该代码的核心是torch.nn.MaxPool2d,并设置kernel_size = 2,这样同样也默认了步长stride = 2. 输出结果如下: tensor([[[[4., 8.], [9., 8.]]]]) 6. CNN网络结构流程举例 输入:(batch, 1, 28, 28) Conv2d Layer 1: filter 5×55 \\times 55×5, CinC_{in}Cin​:1, CoutC_{out}Cout​:10 → (batch, 10, 24, 24) Pooling Layer 1: filter 2×22 \\times 22×2 → (batch, 10, 12, 12) Conv2d Layer 2: filter 5×55 \\times 55×5, CinC_{in}Cin​:10, CoutC_{out}Cout​:20 → (batch, 20, 8, 8) Pooling Layer 2: filter 2×22 \\times 22×2 → (batch, 20, 4, 4) 展平成向量,经过全连接层映射为10类输出. 最终代码实现: import torch from torchvision import transforms, datasets from torch.utils.data import DataLoader import torch.nn.functional as F import torch.optim as optim batch_size = 64 transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ]) train_dataset = datasets.MNIST( root='D:/PythonCode/Pytorch_learning/MNIST', train=True, download=True, transform=transform ) train_loader = DataLoader( train_dataset, batch_size=batch_size, shuffle=True ) test_dataset = datasets.MNIST( root='D:/PythonCode/Pytorch_learning/MNIST', train=False, download=True, transform=transform ) test_loader = DataLoader( test_dataset, batch_size=batch_size, shuffle=False ) class Net(torch.nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=5) self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=5) self.pooling = torch.nn.MaxPool2d(2) self.fc = torch.nn.Linear(320, 10) def forward(self, x): batch_size = x.size(0) x = F.relu(self.pooling(self.conv1(x))) x = F.relu(self.pooling(self.conv2(x))) x = x.view(batch_size, -1) x = self.fc(x) return x model = Net() criterion = torch.nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5) def train(epoch): running_loss = 0.0 for batch_idx, data in enumerate(train_loader, 0): inputs, target = data optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, target) loss.backward() optimizer.step() running_loss += loss.item() if batch_idx % 300 == 299: print('[%d, %5d] loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / 300)) running_loss = 0.0 def test(): correct = 0 total = 0 with torch.no_grad(): for data in test_loader: inputs, labels = data outputs = model(inputs) _, predicted = torch.max(outputs.data, dim = 1) total += labels.size(0) correct += (predicted == labels).sum().item() print('Accuracy on test set: %d %% [%d/%d]' % (100 * correct / total, correct, total)) if __name__ == '__main__': for epoch in range(10): train(epoch) test() 本笔记系统梳理了卷积神经网络的输入结构,卷积与池化操作的原理,参数设置,数学推导与代码实现,并通过具体的网络结构流程示例,帮助理解CNN的整体信息流与特征提取机制.","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://toddy110.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://toddy110.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"author":"Modesty"}],"categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://toddy110.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"深度学习","slug":"深度学习","permalink":"https://toddy110.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://toddy110.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"深度学习","slug":"深度学习","permalink":"https://toddy110.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]}